{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18d5aca",
   "metadata": {},
   "source": [
    "User Query\n",
    "   ‚Üì\n",
    "Agent 1 (PDF RAG)\n",
    "\n",
    "   ‚îú‚îÄ Answer found ‚Üí RETURN\n",
    "\n",
    "   ‚îî‚îÄ Not found ‚Üí Trigger Agent 2 \n",
    "   \n",
    "                     ‚Üì\n",
    "               Agent 2 (Wikipedia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883af833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key= GROQ_API_KEY,temperature=0.7\n",
    ")\n",
    "\n",
    "# llm = LLM(\n",
    "#     model = \"huggingface/meta-llama/Llama-3.3-70B-Instruct\",\n",
    "#     # model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \n",
    "#     api_key= os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/aiapp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Crew, Task, LLM\n",
    "from crewai_tools import PDFSearchTool\n",
    "\n",
    "\n",
    "# PDFSearchTool ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶® - ‡¶è‡¶ü‡¶ø ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§\n",
    "pdf_tool = PDFSearchTool(\n",
    "    pdf='ragagent/data/MachineLearning.pdf',\n",
    "    config={\n",
    "        \"embedding_model\": {\n",
    "            \"provider\": \"sentence-transformer\",\n",
    "            \"config\": {\"model\": \"all-MiniLM-L6-v2\"}\n",
    "        },\n",
    "        \"vectordb\": {\n",
    "            \"provider\": \"chromadb\",\n",
    "            \"config\": {}\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90147152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrewAI storage location: /home/ahmed/snap/code/219/.local/share/Agent\n",
      "üìÅ chroma.sqlite3\n",
      "üìÅ chromadb-b6e1f73bd8c9480e42e90856fc17defb.lock\n",
      "üìÅ .crewai_user.json\n",
      "üìÅ ee9e4d22-5caf-4327-a98e-5574ad9bf99c\n",
      "üìÅ latest_kickoff_task_outputs.db\n"
     ]
    }
   ],
   "source": [
    "from crewai.utilities.paths import db_storage_path\n",
    "import os\n",
    "\n",
    "storage_path = db_storage_path()\n",
    "print(f\"CrewAI storage location: {storage_path}\")\n",
    "\n",
    "# ‡¶∏‡¶¨ ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®\n",
    "if os.path.exists(storage_path):\n",
    "    for item in os.listdir(storage_path):\n",
    "        print(f\"üìÅ {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39803d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from crewai.utilities.paths import db_storage_path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# CrewAI storage path ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®\n",
    "storage_path = db_storage_path()\n",
    "print(f\"Storage path: {storage_path}\")\n",
    "\n",
    "# # ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ storage ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®\n",
    "# if os.path.exists(storage_path):\n",
    "#     shutil.rmtree(storage_path)\n",
    "#     print(\"‚úÖ Storage deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb92347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrewAI storage location: /home/ahmed/snap/code/219/.local/share/Agent\n",
      "üìÅ chroma.sqlite3\n",
      "üìÅ chromadb-b6e1f73bd8c9480e42e90856fc17defb.lock\n",
      "üìÅ .crewai_user.json\n",
      "üìÅ ee9e4d22-5caf-4327-a98e-5574ad9bf99c\n",
      "üìÅ latest_kickoff_task_outputs.db\n"
     ]
    }
   ],
   "source": [
    "from crewai.utilities.paths import db_storage_path\n",
    "import os\n",
    "\n",
    "storage_path = db_storage_path()\n",
    "print(f\"CrewAI storage location: {storage_path}\")\n",
    "\n",
    "# ‡¶∏‡¶¨ ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®\n",
    "if os.path.exists(storage_path):\n",
    "    for item in os.listdir(storage_path):\n",
    "        print(f\"üìÅ {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f6d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    role=\"PDF Knowledge Expert\",\n",
    "    goal=\"Answer questions based on PDF documents.\",\n",
    "    backstory=\"An AI assistant that extracts information from PDFs.\",\n",
    "    llm=llm,\n",
    "    tools=[pdf_tool],  # tools ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡¶ø‡¶®, knowledge_sources ‡¶®‡¶Ø‡¶º\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Task\n",
    "# -------------------------\n",
    "pdf_task = Task(\n",
    "    description=(\n",
    "        \"Answer the user query using ONLY the PDF content.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1. Do NOT use prior knowledge\\n\"\n",
    "        \"2. Do NOT guess\\n\"\n",
    "        \"3. If the PDF does not contain the answer, respond with: NOT_FOUND\\n\\n\"\n",
    "        \"User Question: {input}\"\n",
    "    ),\n",
    "    agent=agent,\n",
    "    expected_output=\"Answer from PDF or NOT_FOUND\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Crew\n",
    "# -------------------------\n",
    "pdf_crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[pdf_task],\n",
    "    # verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3 Decision Tree Learning\\nA decision tree is an acyclic graph that can be used to make decisions. In each branching\\nnode of the graph, a specific feature j of the feature vector is examined. If the value of the\\nfeature is below a specific threshold, then the left branch is followed; otherwise, the right\\nbranch is followed. As the leaf node is reached, the decision is made about the class to which\\nthe example belongs.\\nAs the title of the section suggests, a decision tree can be learned from data.\\n3.3.1 Problem Statement\\nLike previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We\\nwant to build a decision tree that would allow us to predict the class of an example given a\\nfeature vector.\\n3.3.2 Solution\\nThere are various formulations of the decision tree learning algorithm. In this book, we\\nconsider just one, called ID3.\\nThe optimization criterion, in this case, is the average log-likelihood:\\n1\\nN\\nN\\n√ø\\ni=1\\nyi ln fID3(xi) + (1 ‚â†yi) ln (1 ‚â†fID3(xi)),\\n(5)\\nwhere fID3 is a decision tree.\\nBy now, it looks very similar to logistic regression. However, contrary to the logistic regression\\nlearning algorithm which builds a parametric model fw,b by finding an optimal solution\\nto the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a\\nnon-parametric model fID3(x)\\ndef\\n= Pr(y = 1|x).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9\\nPage 36:\\nS = {(x1, y1), (x2, y2), (x3, y3),\\n(x4, y4), (x5, y5), (x6, y6),\\n(x7, y7), (x8, y8), (x9, y9),\\n(x10, y10), (x11, y11), (x12, y12)}\\nx\\nPr(y = 1|x) = (y1+y2+y3+y4+y5\\n+y6+y7+y8+y9+y10+y11+y12)/12\\nPr(y = 1|x)\\n(a)\\nx\\nPr(y = 1|x) = (y1+y2+y4\\n+y6+y7+y8+y9)/7\\nPr(y = 1|x)\\nx(3) < 18.3?\\nS‚â† = {(x1, y1), (x2, y2),\\n(x4, y4), (x6, y6), (x7, y7),\\n(x8, y8), (x9, y9)}\\nPr(y = 1|x) =\\n(y3+y5+y10+y11+y12)/5\\nPr(y = 1|x)\\nS+ = {(x3, y3), (x5, y5), (x10, y10),\\n(x11, y11), (x12, y12)}\\nYes\\nNo\\n(b)\\nFigure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled\\nexamples. (a) In the beginning, the decision tree only contains the start node; it makes the\\nsame prediction for any input. (b) The decision tree after the first split; it tests whether\\nfeature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the\\ntwo leaf nodes.\\nThe ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the\\nbeginning, the decision tree only has a start node that contains all examples: S\\ndef\\n= {(xi, yi)}Ni=1.\\nStart with a constant model fS\\nID3:\\nfS\\nID3 = 1\\n|S|\\n√ø\\n(x,y)‚ààS\\ny.\\n(6)\\nThe prediction given by the above model, fS\\nID3(x), would be the same for any input x. The\\ncorresponding decision tree is shown in fig 4a.\\nThen we search through all features j = 1, . . . , D and all thresholds t, and split the set S\\ninto two subsets: S‚â†\\ndef\\n= {(xi, yi) ‚àà S | xji < t}\\nand S+\\ndef\\n= {(xi, yi) ‚àà S | xji ‚â• t}.\\nWe split the set in such a way that the likelihood of the data given the tree is maximized. In\\nother words, we are looking for the split for which\\n√ø\\n(x,y)‚ààS‚â†\\n(y ln fS‚â†\\nID3(x) + (1 ‚â† y) ln (1 ‚â† fS‚â†\\nID3(x))) + √ø\\n(x,y)‚ààS+\\n(y ln fS+\\nID3(x) + (1 ‚â† y) ln (1 ‚â† fS+\\nID3(x)))\\nis greatest. The constants fS‚â†\\nID3 and fS+\\nID3 are computed, as before, using eq. (6). Once the\\nsplit is made, we have two subsets, S‚â† and S+, and two predicted probabilities, fS‚â†\\nID3 and\\nfS+\\nID3\\n, one for each subset. These predicted probabilities are, as before, constant. They correspond\\nto two new leaf nodes, that are added to the tree (see fig 4b). Note that the two predicted\\nprobabilities are usually different, which is why the split was useful in the first place.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pdf_crew.kickoff(inputs={\"input\": \"Decision Tree Learning?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.3 Semi-Supervised Learning\\n\\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples.\\n\\nUsually, the quantity of unlabeled examples is much higher than the number of labeled\\n\\nexamples. The goal of a semi-supervised learning algorithm is the same as the goal of\\n\\nthe supervised learning algorithm. The hope here is that using many unlabeled examples can\\n\\nhelp the learning algorithm to Ô¨Ånd (we might say ‚Äúproduce‚Äù or ‚Äúcompute‚Äù) a better model2.\\n\\n2It could look counter-intuitive that learning could beneÔ¨Åt from adding more unlabeled examples. It seems\\n\\nlike we add more uncertainty to the problem. However, when you add unlabeled examples, you add more\\n\\ninformation about your problem: a larger sample reÔ¨Çects better the probability distribution the data we\\n\\nlabeled came from. Theoretically, a learning algorithm should be able to leverage this additional information.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pdf_crew.kickoff(inputs={\"input\": \"Semi-Supervised Learning?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c16d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT_FOUND'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pdf_crew.kickoff(inputs={\"input\": \"whats is agentic ai?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c0abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3\\n\\nFundamental Algorithms\\n\\nIn this chapter, I describe Ô¨Åve algorithms which are not just the most known but also either\\n\\nvery eÔ¨Äective on their own or are used as building blocks for the most eÔ¨Äective learning\\n\\nalgorithms out there.\\n\\n3.1\\n\\nLinear Regression\\n\\nLinear regression is a popular regression learning algorithm that learns a model which is a\\n\\nlinear combination of features of the input example.\\n\\n3.1.1\\n\\nProblem Statement\\n\\nWe have a collection of labeled examples {(xi, yi)}N\\n\\ni=1, where N is the size of the collection,\\n\\nxi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target\\n\\nand every feature x(j)\\n\\ni , j = 1, . . . , D, is also a real number.\\n\\nWe want to build a model fw,b(x) as a linear combination of features of example x:\\n\\nfw,b(x) = wx + b,\\n\\n(1)\\n\\nwhere w is a D-dimensional vector of parameters and b is a real number. The notation fw,b\\n\\nmeans that the model f is parametrized by two values: w and b.\\n\\nWe will use the model to predict the unknown y for a given x like this: y Œ© fw,b(x). Two\\n\\nmodels parametrized by two diÔ¨Äerent pairs (w, b) will likely produce two diÔ¨Äerent predictions\\n\\nwhen applied to the same example. We want to Ô¨Ånd the optimal values (w√∫, b√∫). Obviously,\\n\\nthe optimal values of parameters deÔ¨Åne the model that makes the most accurate predictions.\\n\\nYou could have noticed that the form of our linear model in eq. 1 is very similar to the form\\n\\nof the SVM model. The only diÔ¨Äerence is the missing sign operator. The two models are\\n\\nindeed similar. However, the hyperplane in the SVM plays the role of the decision boundary:\\n\\nit‚Äôs used to separate two groups of examples from one another. As such, it has to be as far\\n\\nfrom each group as possible.\\n\\nOn the other hand, the hyperplane in linear regression is chosen to be as close to all training\\n\\nexamples as possible.\\n\\nYou can see why this latter requirement is essential by looking at the illustration in Ô¨Åg. 1. It\\n\\ndisplays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We\\n\\ncan use this line to predict the value of the target ynew for a new unlabeled input example\\n\\nxnew. If our examples are D-dimensional feature vectors (for D > 1), the only diÔ¨Äerence\\n\\n1To say that yi is real-valued, we write yi ≈ì R, where R denotes the set of all real numbers, an inÔ¨Ånite set\\n\\nof numbers from minus inÔ¨Ånity to plus inÔ¨Ånity.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\nPage 30:\\nFigure 1: Linear Regression for one-dimensional examples.\\nwith the one-dimensional case is that the regression model is not a line but a plane (for two\\ndimensions) or a hyperplane (for D > 2).\\nNow you see why it‚Äôs essential to have the requirement that the regression hyperplane lies as\\nclose to the training examples as possible: if the blue line in Ô¨Åg. 1 was far from the blue dots,\\nthe prediction ynew would have fewer chances to be correct.\\n3.1.2\\nSolution\\nTo get this latter requirement satisÔ¨Åed, the optimization procedure which we use to Ô¨Ånd the\\noptimal values for w√∫ and b√∫ tries to minimize the following expression:\\n1\\nN\\n√ø\\ni=1...N\\n(fw,b(xi) ‚â†yi)2.\\n(2)\\nIn mathematics, the expression we minimize or maximize is called an objective function, or,\\nsimply, an objective. The expression (f(xi) ‚â†yi)2 in the above objective is called the loss\\nfunction. It‚Äôs a measure of penalty for misclassiÔ¨Åcation of example i. This particular choice\\nof the loss function is called squared error loss. All model-based learning algorithms have\\na loss function and what we do to Ô¨Ånd the best model is we try to minimize the objective\\nknown as the cost function. In linear regression, the cost function is given by the average\\nloss, also called the empirical risk. The average loss, or empirical risk, for a model, is the\\naverage of all penalties obtained by applying the model to the training data.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pdf_crew.kickoff(inputs={\"input\": \"whats is Linear Regression?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433dcce",
   "metadata": {},
   "source": [
    "## agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4434360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# # Example: Flan-T5 large (free hosted)\n",
    "# llm = LLM(\n",
    "#     model=\"google/flan-t5-large\",\n",
    "#     api_key=HUGGINGFACEHUB_API_TOKEN,\n",
    "#     temperature=0,   # deterministic output\n",
    "#     max_tokens=1000  # optional\n",
    "# )\n",
    "llm = LLM(\n",
    "    model = \"huggingface/meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    # model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    api_key= os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    temperature=0,   # deterministic output\n",
    "    max_tokens=1000  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from crewai.tools import tool\n",
    "from crewai_tools import SerperDevTool\n",
    "import wikipedia\n",
    "\n",
    "serper_search = SerperDevTool()\n",
    "\n",
    "@tool(\"Wikipedia Search\")\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches Wikipedia and returns a short summary (up to 4 sentences).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=4)\n",
    "        return summary\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # If multiple options exist, pick first\n",
    "        return wikipedia.summary(e.options[0], sentences=4)\n",
    "    except Exception:\n",
    "        return \"NOT_FOUND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf79f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Crew, Task, LLM\n",
    "\n",
    "# -------------------------\n",
    "# Wikipedia Agent\n",
    "# -------------------------\n",
    "wikipedia_agent = Agent(\n",
    "    llm=llm,\n",
    "    role=\"Wikipedia Research Agent\",\n",
    "    goal=\"Answer user questions using Wikipedia if PDF agent did not find an answer\",\n",
    "    backstory=(\n",
    "        \"You are an expert in summarizing Wikipedia content. \"\n",
    "        \"Always give short, factual answers. \"\n",
    "        \"If the topic is not found, respond with exactly: NOT_FOUND\"\n",
    "    ),\n",
    "    tools=[wikipedia_search],\n",
    "    # verbose=True,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Task\n",
    "# -------------------------\n",
    "wikipedia_task = Task(\n",
    "    description=(\n",
    "        \"Answer the user query using Wikipedia content.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1. Search Wikipedia using the wikipedia_search tool.\\n\"\n",
    "        \"2. Provide concise and factual summary.\\n\"\n",
    "        \"3. If the topic is not found, respond with: NOT_FOUND\\n\"\n",
    "        \"User Question: {input}\"\n",
    "    ),\n",
    "    agent=wikipedia_agent,\n",
    "    expected_output=\"Answer from Wikipedia or NOT_FOUND\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Crew\n",
    "# -------------------------\n",
    "wikipedia_crew = Crew(\n",
    "    agents=[wikipedia_agent],\n",
    "    tasks=[wikipedia_task],\n",
    "    # verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In statistics, linear regression is a model that estimates the relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression. This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable.\\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the relationship between two or more variables, and the resulting model is used to predict the value of one variable given the values of the others.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wikipedia_crew.kickoff(inputs={\"input\": \"whats is Linear Regression?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wikipedia_crew.kickoff(inputs={\"input\": \"whats is agentic ai?\"})\n",
    "result.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76525fc4",
   "metadata": {},
   "source": [
    "## combine 2 agent def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fa0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combined_crew(query: str):\n",
    "    # Run PDF Agent first\n",
    "    pdf_result = pdf_crew.kickoff(inputs={\"input\": query})\n",
    "    pdf_answer = pdf_result.raw.strip()\n",
    "\n",
    "    if pdf_answer == \"NOT_FOUND\":\n",
    "        # Fallback to Wikipedia Agent\n",
    "        wiki_result = wikipedia_crew.kickoff(inputs={\"input\": query})\n",
    "        return {\n",
    "            \"source\": \"Wikipedia\",\n",
    "            \"answer\": wiki_result.raw\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"source\": \"PDF\",\n",
    "            \"answer\": pdf_answer\n",
    "        }\n",
    "\n",
    "result = run_combined_crew(\"whats is agentic ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f3162a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Wikipedia',\n",
       " 'answer': 'There is no Wikipedia page on \"Agentic AI\" with a detailed content, however, In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\\n\\n== Overview ==\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032bb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb175d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
